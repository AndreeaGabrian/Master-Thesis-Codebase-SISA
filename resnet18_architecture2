digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2428228577984 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	2428228616096 [label=AddmmBackward0]
	2428228615856 -> 2428228616096
	2428117323984 [label="fc.bias
 (1000)" fillcolor=lightblue]
	2428117323984 -> 2428228615856
	2428228615856 [label=AccumulateGrad]
	2428228602560 -> 2428228616096
	2428228602560 [label=ViewBackward0]
	2428228602512 -> 2428228602560
	2428228602512 [label=MeanBackward1]
	2428228615664 -> 2428228602512
	2428228615664 [label=ReluBackward0]
	2428228615760 -> 2428228615664
	2428228615760 [label=AddBackward0]
	2428228615520 -> 2428228615760
	2428228615520 [label=NativeBatchNormBackward0]
	2428228615376 -> 2428228615520
	2428228615376 [label=ConvolutionBackward0]
	2428228615088 -> 2428228615376
	2428228615088 [label=ReluBackward0]
	2428228613936 -> 2428228615088
	2428228613936 [label=NativeBatchNormBackward0]
	2428228613456 -> 2428228613936
	2428228613456 [label=ConvolutionBackward0]
	2428228615808 -> 2428228613456
	2428228615808 [label=ReluBackward0]
	2428228611728 -> 2428228615808
	2428228611728 [label=AddBackward0]
	2428228611104 -> 2428228611728
	2428228611104 [label=NativeBatchNormBackward0]
	2428228610528 -> 2428228611104
	2428228610528 [label=ConvolutionBackward0]
	2428228609424 -> 2428228610528
	2428228609424 [label=ReluBackward0]
	2428228608464 -> 2428228609424
	2428228608464 [label=NativeBatchNormBackward0]
	2428228608080 -> 2428228608464
	2428228608080 [label=ConvolutionBackward0]
	2428228607456 -> 2428228608080
	2428228607456 [label=ReluBackward0]
	2428228607024 -> 2428228607456
	2428228607024 [label=AddBackward0]
	2428228606592 -> 2428228607024
	2428228606592 [label=NativeBatchNormBackward0]
	2428228606304 -> 2428228606592
	2428228606304 [label=ConvolutionBackward0]
	2428228605680 -> 2428228606304
	2428228605680 [label=ReluBackward0]
	2428228605296 -> 2428228605680
	2428228605296 [label=NativeBatchNormBackward0]
	2428228604816 -> 2428228605296
	2428228604816 [label=ConvolutionBackward0]
	2428228606736 -> 2428228604816
	2428228606736 [label=ReluBackward0]
	2428228602368 -> 2428228606736
	2428228602368 [label=AddBackward0]
	2428228603184 -> 2428228602368
	2428228603184 [label=NativeBatchNormBackward0]
	2428228615232 -> 2428228603184
	2428228615232 [label=ConvolutionBackward0]
	2428228786240 -> 2428228615232
	2428228786240 [label=ReluBackward0]
	2428228786000 -> 2428228786240
	2428228786000 [label=NativeBatchNormBackward0]
	2428228785904 -> 2428228786000
	2428228785904 [label=ConvolutionBackward0]
	2428228785712 -> 2428228785904
	2428228785712 [label=ReluBackward0]
	2428228785664 -> 2428228785712
	2428228785664 [label=AddBackward0]
	2428228785568 -> 2428228785664
	2428228785568 [label=NativeBatchNormBackward0]
	2428228785328 -> 2428228785568
	2428228785328 [label=ConvolutionBackward0]
	2428228785136 -> 2428228785328
	2428228785136 [label=ReluBackward0]
	2428228785088 -> 2428228785136
	2428228785088 [label=NativeBatchNormBackward0]
	2428228784992 -> 2428228785088
	2428228784992 [label=ConvolutionBackward0]
	2428228785520 -> 2428228784992
	2428228785520 [label=ReluBackward0]
	2428228784704 -> 2428228785520
	2428228784704 [label=AddBackward0]
	2428228784608 -> 2428228784704
	2428228784608 [label=NativeBatchNormBackward0]
	2428228784368 -> 2428228784608
	2428228784368 [label=ConvolutionBackward0]
	2428228784176 -> 2428228784368
	2428228784176 [label=ReluBackward0]
	2428228784128 -> 2428228784176
	2428228784128 [label=NativeBatchNormBackward0]
	2428228784032 -> 2428228784128
	2428228784032 [label=ConvolutionBackward0]
	2428228783840 -> 2428228784032
	2428228783840 [label=ReluBackward0]
	2428228783600 -> 2428228783840
	2428228783600 [label=AddBackward0]
	2428228783504 -> 2428228783600
	2428228783504 [label=NativeBatchNormBackward0]
	2428228783456 -> 2428228783504
	2428228783456 [label=ConvolutionBackward0]
	2428228783264 -> 2428228783456
	2428228783264 [label=ReluBackward0]
	2428228783024 -> 2428228783264
	2428228783024 [label=NativeBatchNormBackward0]
	2428228782928 -> 2428228783024
	2428228782928 [label=ConvolutionBackward0]
	2428228783648 -> 2428228782928
	2428228783648 [label=ReluBackward0]
	2428228782640 -> 2428228783648
	2428228782640 [label=AddBackward0]
	2428228782544 -> 2428228782640
	2428228782544 [label=NativeBatchNormBackward0]
	2428228782496 -> 2428228782544
	2428228782496 [label=ConvolutionBackward0]
	2428228782304 -> 2428228782496
	2428228782304 [label=ReluBackward0]
	2428228782064 -> 2428228782304
	2428228782064 [label=NativeBatchNormBackward0]
	2428228781968 -> 2428228782064
	2428228781968 [label=ConvolutionBackward0]
	2428228782688 -> 2428228781968
	2428228782688 [label=MaxPool2DWithIndicesBackward0]
	2428228781680 -> 2428228782688
	2428228781680 [label=ReluBackward0]
	2428228781584 -> 2428228781680
	2428228781584 [label=NativeBatchNormBackward0]
	2428228781488 -> 2428228781584
	2428228781488 [label=ConvolutionBackward0]
	2428228781296 -> 2428228781488
	2428117243664 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2428117243664 -> 2428228781296
	2428228781296 [label=AccumulateGrad]
	2428228781632 -> 2428228781584
	2428117243504 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2428117243504 -> 2428228781632
	2428228781632 [label=AccumulateGrad]
	2428228781872 -> 2428228781584
	2428117243424 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2428117243424 -> 2428228781872
	2428228781872 [label=AccumulateGrad]
	2428228781776 -> 2428228781968
	2428117242784 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2428117242784 -> 2428228781776
	2428228781776 [label=AccumulateGrad]
	2428228782112 -> 2428228782064
	2428117242624 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2428117242624 -> 2428228782112
	2428228782112 [label=AccumulateGrad]
	2428228782160 -> 2428228782064
	2428117242704 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2428117242704 -> 2428228782160
	2428228782160 [label=AccumulateGrad]
	2428228782256 -> 2428228782496
	2428117241024 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2428117241024 -> 2428228782256
	2428228782256 [label=AccumulateGrad]
	2428228782448 -> 2428228782544
	2428117241744 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2428117241744 -> 2428228782448
	2428228782448 [label=AccumulateGrad]
	2428228782592 -> 2428228782544
	2428117240864 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2428117240864 -> 2428228782592
	2428228782592 [label=AccumulateGrad]
	2428228782688 -> 2428228782640
	2428228782736 -> 2428228782928
	2428117240224 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2428117240224 -> 2428228782736
	2428228782736 [label=AccumulateGrad]
	2428228783072 -> 2428228783024
	2428117240304 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2428117240304 -> 2428228783072
	2428228783072 [label=AccumulateGrad]
	2428228783120 -> 2428228783024
	2428117240144 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2428117240144 -> 2428228783120
	2428228783120 [label=AccumulateGrad]
	2428228783216 -> 2428228783456
	2428117247984 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2428117247984 -> 2428228783216
	2428228783216 [label=AccumulateGrad]
	2428228783408 -> 2428228783504
	2428117247824 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2428117247824 -> 2428228783408
	2428228783408 [label=AccumulateGrad]
	2428228783552 -> 2428228783504
	2428117247904 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2428117247904 -> 2428228783552
	2428228783552 [label=AccumulateGrad]
	2428228783648 -> 2428228783600
	2428228783792 -> 2428228784032
	2428117327584 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2428117327584 -> 2428228783792
	2428228783792 [label=AccumulateGrad]
	2428228783984 -> 2428228784128
	2428117327664 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2428117327664 -> 2428228783984
	2428228783984 [label=AccumulateGrad]
	2428228784224 -> 2428228784128
	2428117334624 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2428117334624 -> 2428228784224
	2428228784224 [label=AccumulateGrad]
	2428228784320 -> 2428228784368
	2428117334384 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2428117334384 -> 2428228784320
	2428228784320 [label=AccumulateGrad]
	2428228784512 -> 2428228784608
	2428117334464 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2428117334464 -> 2428228784512
	2428228784512 [label=AccumulateGrad]
	2428228784464 -> 2428228784608
	2428117320944 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2428117320944 -> 2428228784464
	2428228784464 [label=AccumulateGrad]
	2428228784560 -> 2428228784704
	2428228784560 [label=NativeBatchNormBackward0]
	2428228783936 -> 2428228784560
	2428228783936 [label=ConvolutionBackward0]
	2428228783840 -> 2428228783936
	2428228783696 -> 2428228783936
	2428117334864 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2428117334864 -> 2428228783696
	2428228783696 [label=AccumulateGrad]
	2428228784272 -> 2428228784560
	2428117321424 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2428117321424 -> 2428228784272
	2428228784272 [label=AccumulateGrad]
	2428228784416 -> 2428228784560
	2428117321344 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2428117321344 -> 2428228784416
	2428228784416 [label=AccumulateGrad]
	2428228784800 -> 2428228784992
	2428117320784 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2428117320784 -> 2428228784800
	2428228784800 [label=AccumulateGrad]
	2428228784944 -> 2428228785088
	2428117334224 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2428117334224 -> 2428228784944
	2428228784944 [label=AccumulateGrad]
	2428228785184 -> 2428228785088
	2428117320704 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2428117320704 -> 2428228785184
	2428228785184 [label=AccumulateGrad]
	2428228785280 -> 2428228785328
	2428117327024 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2428117327024 -> 2428228785280
	2428228785280 [label=AccumulateGrad]
	2428228785472 -> 2428228785568
	2428117320544 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2428117320544 -> 2428228785472
	2428228785472 [label=AccumulateGrad]
	2428228785424 -> 2428228785568
	2428117326944 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2428117326944 -> 2428228785424
	2428228785424 [label=AccumulateGrad]
	2428228785520 -> 2428228785664
	2428228785856 -> 2428228785904
	2428117320144 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2428117320144 -> 2428228785856
	2428228785856 [label=AccumulateGrad]
	2428228786048 -> 2428228786000
	2428117333744 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2428117333744 -> 2428228786048
	2428228786048 [label=AccumulateGrad]
	2428228786096 -> 2428228786000
	2428117320064 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2428117320064 -> 2428228786096
	2428228786096 [label=AccumulateGrad]
	2428228786192 -> 2428228615232
	2428117326384 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2428117326384 -> 2428228786192
	2428228786192 [label=AccumulateGrad]
	2428228786432 -> 2428228603184
	2428117319904 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2428117319904 -> 2428228786432
	2428228786432 [label=AccumulateGrad]
	2428228786384 -> 2428228603184
	2428117326304 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2428117326304 -> 2428228786384
	2428228786384 [label=AccumulateGrad]
	2428228603040 -> 2428228602368
	2428228603040 [label=NativeBatchNormBackward0]
	2428228785808 -> 2428228603040
	2428228785808 [label=ConvolutionBackward0]
	2428228785712 -> 2428228785808
	2428228785760 -> 2428228785808
	2428117326704 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2428117326704 -> 2428228785760
	2428228785760 [label=AccumulateGrad]
	2428228786336 -> 2428228603040
	2428117326624 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2428117326624 -> 2428228786336
	2428228786336 [label=AccumulateGrad]
	2428228786288 -> 2428228603040
	2428117333664 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2428117333664 -> 2428228786288
	2428228786288 [label=AccumulateGrad]
	2428228603472 -> 2428228604816
	2428117326144 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2428117326144 -> 2428228603472
	2428228603472 [label=AccumulateGrad]
	2428228604912 -> 2428228605296
	2428117326224 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2428117326224 -> 2428228604912
	2428228604912 [label=AccumulateGrad]
	2428228605584 -> 2428228605296
	2428117333184 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2428117333184 -> 2428228605584
	2428228605584 [label=AccumulateGrad]
	2428228605824 -> 2428228606304
	2428117325824 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2428117325824 -> 2428228605824
	2428228605824 [label=AccumulateGrad]
	2428228606496 -> 2428228606592
	2428117325904 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2428117325904 -> 2428228606496
	2428228606496 [label=AccumulateGrad]
	2428228606544 -> 2428228606592
	2428117332864 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2428117332864 -> 2428228606544
	2428228606544 [label=AccumulateGrad]
	2428228606736 -> 2428228607024
	2428228607600 -> 2428228608080
	2428117332224 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2428117332224 -> 2428228607600
	2428228607600 [label=AccumulateGrad]
	2428228608320 -> 2428228608464
	2428117325184 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2428117325184 -> 2428228608320
	2428228608320 [label=AccumulateGrad]
	2428228608944 -> 2428228608464
	2428117332144 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2428117332144 -> 2428228608944
	2428228608944 [label=AccumulateGrad]
	2428228609520 -> 2428228610528
	2428117331904 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2428117331904 -> 2428228609520
	2428228609520 [label=AccumulateGrad]
	2428228610624 -> 2428228611104
	2428117324864 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2428117324864 -> 2428228610624
	2428228610624 [label=AccumulateGrad]
	2428228611008 -> 2428228611104
	2428117331824 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2428117331824 -> 2428228611008
	2428228611008 [label=AccumulateGrad]
	2428228611632 -> 2428228611728
	2428228611632 [label=NativeBatchNormBackward0]
	2428228607744 -> 2428228611632
	2428228607744 [label=ConvolutionBackward0]
	2428228607456 -> 2428228607744
	2428228607360 -> 2428228607744
	2428117325584 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2428117325584 -> 2428228607360
	2428228607360 [label=AccumulateGrad]
	2428228610048 -> 2428228611632
	2428117325504 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2428117325504 -> 2428228610048
	2428228610048 [label=AccumulateGrad]
	2428228610144 -> 2428228611632
	2428117332544 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2428117332544 -> 2428228610144
	2428228610144 [label=AccumulateGrad]
	2428228612352 -> 2428228613456
	2428117324544 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2428117324544 -> 2428228612352
	2428228612352 [label=AccumulateGrad]
	2428228613840 -> 2428228613936
	2428117324624 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2428117324624 -> 2428228613840
	2428228613840 [label=AccumulateGrad]
	2428228614560 -> 2428228613936
	2428117331584 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2428117331584 -> 2428228614560
	2428228614560 [label=AccumulateGrad]
	2428228615184 -> 2428228615376
	2428117331424 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2428117331424 -> 2428228615184
	2428228615184 [label=AccumulateGrad]
	2428228615424 -> 2428228615520
	2428117324384 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2428117324384 -> 2428228615424
	2428228615424 [label=AccumulateGrad]
	2428228615472 -> 2428228615520
	2428117331344 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2428117331344 -> 2428228615472
	2428228615472 [label=AccumulateGrad]
	2428228615808 -> 2428228615760
	2428228615904 -> 2428228616096
	2428228615904 [label=TBackward0]
	2428228615712 -> 2428228615904
	2428117331104 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	2428117331104 -> 2428228615712
	2428228615712 [label=AccumulateGrad]
	2428228616096 -> 2428228577984
}
