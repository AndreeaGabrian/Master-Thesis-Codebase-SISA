digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2498439150736 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	2498439252576 [label=AddmmBackward0]
	2498439252864 -> 2498439252576
	2498328597328 [label="fc.bias
 (1000)" fillcolor=lightblue]
	2498328597328 -> 2498439252864
	2498439252864 [label=AccumulateGrad]
	2498439252768 -> 2498439252576
	2498439252768 [label=ViewBackward0]
	2498439252720 -> 2498439252768
	2498439252720 [label=MeanBackward1]
	2498439253056 -> 2498439252720
	2498439253056 [label=ReluBackward0]
	2498439252960 -> 2498439253056
	2498439252960 [label=AddBackward0]
	2498439253200 -> 2498439252960
	2498439253200 [label=NativeBatchNormBackward0]
	2498439253344 -> 2498439253200
	2498439253344 [label=ConvolutionBackward0]
	2498439253536 -> 2498439253344
	2498439253536 [label=ReluBackward0]
	2498439253680 -> 2498439253536
	2498439253680 [label=NativeBatchNormBackward0]
	2498439253776 -> 2498439253680
	2498439253776 [label=ConvolutionBackward0]
	2498439252912 -> 2498439253776
	2498439252912 [label=ReluBackward0]
	2498439254064 -> 2498439252912
	2498439254064 [label=AddBackward0]
	2498439254160 -> 2498439254064
	2498439254160 [label=NativeBatchNormBackward0]
	2498439254304 -> 2498439254160
	2498439254304 [label=ConvolutionBackward0]
	2498439254496 -> 2498439254304
	2498439254496 [label=ReluBackward0]
	2498439254640 -> 2498439254496
	2498439254640 [label=NativeBatchNormBackward0]
	2498439254736 -> 2498439254640
	2498439254736 [label=ConvolutionBackward0]
	2498439254928 -> 2498439254736
	2498439254928 [label=ReluBackward0]
	2498439255072 -> 2498439254928
	2498439255072 [label=AddBackward0]
	2498439255168 -> 2498439255072
	2498439255168 [label=NativeBatchNormBackward0]
	2498439255312 -> 2498439255168
	2498439255312 [label=ConvolutionBackward0]
	2498439255504 -> 2498439255312
	2498439255504 [label=ReluBackward0]
	2498439255648 -> 2498439255504
	2498439255648 [label=NativeBatchNormBackward0]
	2498439255744 -> 2498439255648
	2498439255744 [label=ConvolutionBackward0]
	2498439255120 -> 2498439255744
	2498439255120 [label=ReluBackward0]
	2498439256032 -> 2498439255120
	2498439256032 [label=AddBackward0]
	2498439256128 -> 2498439256032
	2498439256128 [label=NativeBatchNormBackward0]
	2498439256272 -> 2498439256128
	2498439256272 [label=ConvolutionBackward0]
	2498439256464 -> 2498439256272
	2498439256464 [label=ReluBackward0]
	2498439256608 -> 2498439256464
	2498439256608 [label=NativeBatchNormBackward0]
	2498439256704 -> 2498439256608
	2498439256704 [label=ConvolutionBackward0]
	2498439256896 -> 2498439256704
	2498439256896 [label=ReluBackward0]
	2498439257040 -> 2498439256896
	2498439257040 [label=AddBackward0]
	2498439257136 -> 2498439257040
	2498439257136 [label=NativeBatchNormBackward0]
	2498439257280 -> 2498439257136
	2498439257280 [label=ConvolutionBackward0]
	2498439257472 -> 2498439257280
	2498439257472 [label=ReluBackward0]
	2498439257616 -> 2498439257472
	2498439257616 [label=NativeBatchNormBackward0]
	2498439257712 -> 2498439257616
	2498439257712 [label=ConvolutionBackward0]
	2498439257088 -> 2498439257712
	2498439257088 [label=ReluBackward0]
	2498439258000 -> 2498439257088
	2498439258000 [label=AddBackward0]
	2498439258096 -> 2498439258000
	2498439258096 [label=NativeBatchNormBackward0]
	2498439258240 -> 2498439258096
	2498439258240 [label=ConvolutionBackward0]
	2498439258432 -> 2498439258240
	2498439258432 [label=ReluBackward0]
	2498439258576 -> 2498439258432
	2498439258576 [label=NativeBatchNormBackward0]
	2498439258672 -> 2498439258576
	2498439258672 [label=ConvolutionBackward0]
	2498439258864 -> 2498439258672
	2498439258864 [label=ReluBackward0]
	2498439259008 -> 2498439258864
	2498439259008 [label=AddBackward0]
	2498439259104 -> 2498439259008
	2498439259104 [label=NativeBatchNormBackward0]
	2498439259248 -> 2498439259104
	2498439259248 [label=ConvolutionBackward0]
	2498439259440 -> 2498439259248
	2498439259440 [label=ReluBackward0]
	2498439259584 -> 2498439259440
	2498439259584 [label=NativeBatchNormBackward0]
	2498439259680 -> 2498439259584
	2498439259680 [label=ConvolutionBackward0]
	2498439259056 -> 2498439259680
	2498439259056 [label=ReluBackward0]
	2498439259968 -> 2498439259056
	2498439259968 [label=AddBackward0]
	2498439260064 -> 2498439259968
	2498439260064 [label=NativeBatchNormBackward0]
	2498439260208 -> 2498439260064
	2498439260208 [label=ConvolutionBackward0]
	2498439260400 -> 2498439260208
	2498439260400 [label=ReluBackward0]
	2498439260544 -> 2498439260400
	2498439260544 [label=NativeBatchNormBackward0]
	2498439260640 -> 2498439260544
	2498439260640 [label=ConvolutionBackward0]
	2498439260016 -> 2498439260640
	2498439260016 [label=MaxPool2DWithIndicesBackward0]
	2498439260928 -> 2498439260016
	2498439260928 [label=ReluBackward0]
	2498439261024 -> 2498439260928
	2498439261024 [label=NativeBatchNormBackward0]
	2498439261120 -> 2498439261024
	2498439261120 [label=ConvolutionBackward0]
	2498439261312 -> 2498439261120
	2498328525888 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2498328525888 -> 2498439261312
	2498439261312 [label=AccumulateGrad]
	2498439261072 -> 2498439261024
	2498328511568 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2498328511568 -> 2498439261072
	2498439261072 [label=AccumulateGrad]
	2498439260736 -> 2498439261024
	2498328522928 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2498328522928 -> 2498439260736
	2498439260736 [label=AccumulateGrad]
	2498439260832 -> 2498439260640
	2498328514208 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2498328514208 -> 2498439260832
	2498439260832 [label=AccumulateGrad]
	2498439260592 -> 2498439260544
	2498328524928 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2498328524928 -> 2498439260592
	2498439260592 [label=AccumulateGrad]
	2498439260448 -> 2498439260544
	2498328524848 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2498328524848 -> 2498439260448
	2498439260448 [label=AccumulateGrad]
	2498439260352 -> 2498439260208
	2498328515168 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2498328515168 -> 2498439260352
	2498439260352 [label=AccumulateGrad]
	2498439260160 -> 2498439260064
	2498328515008 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2498328515008 -> 2498439260160
	2498439260160 [label=AccumulateGrad]
	2498439260112 -> 2498439260064
	2498328515088 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2498328515088 -> 2498439260112
	2498439260112 [label=AccumulateGrad]
	2498439260016 -> 2498439259968
	2498439259872 -> 2498439259680
	2498328601408 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2498328601408 -> 2498439259872
	2498439259872 [label=AccumulateGrad]
	2498439259632 -> 2498439259584
	2498328521328 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2498328521328 -> 2498439259632
	2498439259632 [label=AccumulateGrad]
	2498439259488 -> 2498439259584
	2498328601328 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2498328601328 -> 2498439259488
	2498439259488 [label=AccumulateGrad]
	2498439259392 -> 2498439259248
	2498328601168 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2498328601168 -> 2498439259392
	2498439259392 [label=AccumulateGrad]
	2498439259200 -> 2498439259104
	2498328601248 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2498328601248 -> 2498439259200
	2498439259200 [label=AccumulateGrad]
	2498439259152 -> 2498439259104
	2498328608288 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2498328608288 -> 2498439259152
	2498439259152 [label=AccumulateGrad]
	2498439259056 -> 2498439259008
	2498439258816 -> 2498439258672
	2498328600768 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2498328600768 -> 2498439258816
	2498439258816 [label=AccumulateGrad]
	2498439258624 -> 2498439258576
	2498328594448 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2498328594448 -> 2498439258624
	2498439258624 [label=AccumulateGrad]
	2498439258480 -> 2498439258576
	2498328600688 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2498328600688 -> 2498439258480
	2498439258480 [label=AccumulateGrad]
	2498439258384 -> 2498439258240
	2498328607648 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2498328607648 -> 2498439258384
	2498439258384 [label=AccumulateGrad]
	2498439258192 -> 2498439258096
	2498328600528 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2498328600528 -> 2498439258192
	2498439258192 [label=AccumulateGrad]
	2498439258144 -> 2498439258096
	2498328607568 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2498328607568 -> 2498439258144
	2498439258144 [label=AccumulateGrad]
	2498439258048 -> 2498439258000
	2498439258048 [label=NativeBatchNormBackward0]
	2498439258768 -> 2498439258048
	2498439258768 [label=ConvolutionBackward0]
	2498439258864 -> 2498439258768
	2498439258912 -> 2498439258768
	2498328608128 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2498328608128 -> 2498439258912
	2498439258912 [label=AccumulateGrad]
	2498439258336 -> 2498439258048
	2498328608048 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2498328608048 -> 2498439258336
	2498439258336 [label=AccumulateGrad]
	2498439258288 -> 2498439258048
	2498328594688 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2498328594688 -> 2498439258288
	2498439258288 [label=AccumulateGrad]
	2498439257904 -> 2498439257712
	2498328607408 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2498328607408 -> 2498439257904
	2498439257904 [label=AccumulateGrad]
	2498439257664 -> 2498439257616
	2498328607488 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2498328607488 -> 2498439257664
	2498439257664 [label=AccumulateGrad]
	2498439257520 -> 2498439257616
	2498328594048 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2498328594048 -> 2498439257520
	2498439257520 [label=AccumulateGrad]
	2498439257424 -> 2498439257280
	2498328593648 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2498328593648 -> 2498439257424
	2498439257424 [label=AccumulateGrad]
	2498439257232 -> 2498439257136
	2498328593728 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2498328593728 -> 2498439257232
	2498439257232 [label=AccumulateGrad]
	2498439257184 -> 2498439257136
	2498328600288 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2498328600288 -> 2498439257184
	2498439257184 [label=AccumulateGrad]
	2498439257088 -> 2498439257040
	2498439256848 -> 2498439256704
	2498328593408 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2498328593408 -> 2498439256848
	2498439256848 [label=AccumulateGrad]
	2498439256656 -> 2498439256608
	2498328606768 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2498328606768 -> 2498439256656
	2498439256656 [label=AccumulateGrad]
	2498439256512 -> 2498439256608
	2498328593328 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2498328593328 -> 2498439256512
	2498439256512 [label=AccumulateGrad]
	2498439256416 -> 2498439256272
	2498328599488 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2498328599488 -> 2498439256416
	2498439256416 [label=AccumulateGrad]
	2498439256224 -> 2498439256128
	2498328593168 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2498328593168 -> 2498439256224
	2498439256224 [label=AccumulateGrad]
	2498439256176 -> 2498439256128
	2498328599408 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2498328599408 -> 2498439256176
	2498439256176 [label=AccumulateGrad]
	2498439256080 -> 2498439256032
	2498439256080 [label=NativeBatchNormBackward0]
	2498439256800 -> 2498439256080
	2498439256800 [label=ConvolutionBackward0]
	2498439256896 -> 2498439256800
	2498439256944 -> 2498439256800
	2498328599968 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2498328599968 -> 2498439256944
	2498439256944 [label=AccumulateGrad]
	2498439256368 -> 2498439256080
	2498328599888 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2498328599888 -> 2498439256368
	2498439256368 [label=AccumulateGrad]
	2498439256320 -> 2498439256080
	2498328607008 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2498328607008 -> 2498439256320
	2498439256320 [label=AccumulateGrad]
	2498439255936 -> 2498439255744
	2498328599248 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2498328599248 -> 2498439255936
	2498439255936 [label=AccumulateGrad]
	2498439255696 -> 2498439255648
	2498328599328 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2498328599328 -> 2498439255696
	2498439255696 [label=AccumulateGrad]
	2498439255552 -> 2498439255648
	2498328606368 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2498328606368 -> 2498439255552
	2498439255552 [label=AccumulateGrad]
	2498439255456 -> 2498439255312
	2498328606128 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2498328606128 -> 2498439255456
	2498439255456 [label=AccumulateGrad]
	2498439255264 -> 2498439255168
	2498328606208 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2498328606208 -> 2498439255264
	2498439255264 [label=AccumulateGrad]
	2498439255216 -> 2498439255168
	2498328592768 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2498328592768 -> 2498439255216
	2498439255216 [label=AccumulateGrad]
	2498439255120 -> 2498439255072
	2498439254880 -> 2498439254736
	2498328605648 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2498328605648 -> 2498439254880
	2498439254880 [label=AccumulateGrad]
	2498439254688 -> 2498439254640
	2498328605728 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2498328605728 -> 2498439254688
	2498439254688 [label=AccumulateGrad]
	2498439254544 -> 2498439254640
	2498328598528 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2498328598528 -> 2498439254544
	2498439254544 [label=AccumulateGrad]
	2498439254448 -> 2498439254304
	2498328605328 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2498328605328 -> 2498439254448
	2498439254448 [label=AccumulateGrad]
	2498439254256 -> 2498439254160
	2498328605408 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2498328605408 -> 2498439254256
	2498439254256 [label=AccumulateGrad]
	2498439254208 -> 2498439254160
	2498328598208 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2498328598208 -> 2498439254208
	2498439254208 [label=AccumulateGrad]
	2498439254112 -> 2498439254064
	2498439254112 [label=NativeBatchNormBackward0]
	2498439254832 -> 2498439254112
	2498439254832 [label=ConvolutionBackward0]
	2498439254928 -> 2498439254832
	2498439254976 -> 2498439254832
	2498328592608 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2498328592608 -> 2498439254976
	2498439254976 [label=AccumulateGrad]
	2498439254400 -> 2498439254112
	2498328592528 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2498328592528 -> 2498439254400
	2498439254400 [label=AccumulateGrad]
	2498439254352 -> 2498439254112
	2498328598848 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2498328598848 -> 2498439254352
	2498439254352 [label=AccumulateGrad]
	2498439253968 -> 2498439253776
	2498328605088 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2498328605088 -> 2498439253968
	2498439253968 [label=AccumulateGrad]
	2498439253728 -> 2498439253680
	2498328597968 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2498328597968 -> 2498439253728
	2498439253728 [label=AccumulateGrad]
	2498439253584 -> 2498439253680
	2498328605008 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2498328605008 -> 2498439253584
	2498439253584 [label=AccumulateGrad]
	2498439253488 -> 2498439253344
	2498328604928 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2498328604928 -> 2498439253488
	2498439253488 [label=AccumulateGrad]
	2498439253296 -> 2498439253200
	2498328597808 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2498328597808 -> 2498439253296
	2498439253296 [label=AccumulateGrad]
	2498439253248 -> 2498439253200
	2498328604848 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2498328604848 -> 2498439253248
	2498439253248 [label=AccumulateGrad]
	2498439252912 -> 2498439252960
	2498439252816 -> 2498439252576
	2498439252816 [label=TBackward0]
	2498439253008 -> 2498439252816
	2498328597408 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	2498328597408 -> 2498439253008
	2498439253008 [label=AccumulateGrad]
	2498439252576 -> 2498439150736
}
